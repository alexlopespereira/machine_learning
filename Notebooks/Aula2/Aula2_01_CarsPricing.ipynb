{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "name": "Classificacao_Preco_Carros_Rede_Neural",
    "language_info": {
      "name": "python",
      "version": ""
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlopespereira/machine_learning/blob/main/Notebooks/Aula2/Aula2_01_CarsPricing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzFfY9V3FDOJ"
      },
      "source": [
        "# Classificação de Preço de Automóveis em Faixas\n",
        "### Usando Rede Neural (MLPClassifier) do scikit-learn\n",
        "\n",
        "Este *notebook* realiza a classificação do preço de automóveis em três faixas (baixo, médio, alto), definidas pelos percentis de 33% e 66%. São realizadas as seguintes etapas:\n",
        "\n",
        "1. **Carregamento** e **exploração inicial** dos dados.\n",
        "2. **Limpeza** de dados (tratamento de valores faltantes, conversão de tipos).\n",
        "3. **Categorização** da variável *price* em faixas.\n",
        "4. **Análise exploratória** simples (histograma e mapa de calor de correlação).\n",
        "5. **Conversão** de variáveis categóricas em *dummies* (One-Hot Encoding).\n",
        "6. **Normalização** das variáveis numéricas (MinMaxScaler).\n",
        "7. **Seleção de variáveis** (RFE) usando *LogisticRegression* como estimador base.\n",
        "8. **Rede Neural MLP** para classificação (MLPClassifier do scikit-learn).\n",
        "9. **Avaliação** do modelo (matriz de confusão, relatório de classificação, acurácia).\n",
        "\n",
        "> **Observação**: O RFE utiliza `LogisticRegression` internamente, pois o `MLPClassifier` não disponibiliza coeficientes/atributos de importância de forma direta para a seleção de variáveis.\n",
        "\n",
        "Vamos iniciar o processo!"
      ],
      "id": "ZzFfY9V3FDOJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvm9g9XVFDOM"
      },
      "source": [
        "## 1. Importação das Bibliotecas\n",
        "*Nesta etapa, importamos as bibliotecas Python necessárias para o projeto.*\n",
        "\n",
        "**Detalhes sobre algumas funções**:\n",
        "- `warnings.filterwarnings('ignore')`: suprime avisos que podem poluir a saída.\n",
        "- `pandas` (`pd`): manipulação de dataframes.\n",
        "- `numpy` (`np`): operações matemáticas/vetoriais.\n",
        "- `matplotlib.pyplot` (`plt`) e `seaborn` (`sns`): geração de gráficos e visualizações.\n",
        "- `LogisticRegression`: modelo linear usado aqui somente para o processo de RFE.\n",
        "- `MLPClassifier`: rede neural *Multilayer Perceptron* para classificação.\n",
        "- `RFE`: *Recursive Feature Elimination*, método de seleção de variáveis.\n",
        "- `MinMaxScaler`: normalização dos dados para o intervalo [0,1].\n",
        "- `train_test_split`: separa dados em conjunto de treino e teste.\n",
        "- `classification_report`, `confusion_matrix`, `accuracy_score`: métricas de avaliação."
      ],
      "id": "uvm9g9XVFDOM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q1T7jUJFDOM"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9q1T7jUJFDOM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr2qnm-TFDON"
      },
      "source": [
        "## 2. Carregamento e Breve Exploração do Dataset\n",
        "*Nesta seção, carregamos o arquivo CSV com os dados dos automóveis e fazemos uma exploração inicial.*\n",
        "\n",
        "**Funções e argumentos**:\n",
        "- `pd.read_csv('Automobile.csv')`: lê o arquivo CSV e retorna um dataframe.\n",
        "- `df.head(10)`: exibe as primeiras 10 linhas do dataframe.\n",
        "- `df.info()`: mostra resumo das colunas, tipos e quantidade de dados não-nulos.\n",
        "- `df.describe()`: gera estatísticas descritivas para colunas numéricas."
      ],
      "id": "hr2qnm-TFDON"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeXJhJVoFDOO"
      },
      "source": [
        "# Carregar o arquivo CSV (ajuste o caminho se necessário)\n",
        "df = pd.read_csv('Automobile.csv')\n",
        "\n",
        "print(\"Amostra do dataset original:\")\n",
        "display(df.head(10))\n",
        "\n",
        "print(\"\\nInformações do dataset:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nDescrição estatística das colunas numéricas:\")\n",
        "display(df.describe())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "GeXJhJVoFDOO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pfjC-MhFDOO"
      },
      "source": [
        "## 3. Limpeza de Dados\n",
        "*Substituímos valores faltantes (`?`) por `NaN`, removemos linhas com dados ausentes e convertemos colunas numéricas.*\n",
        "\n",
        "**Funções e argumentos**:\n",
        "- `df.replace('?', np.nan, inplace=True)`: substitui `?` por `NaN` no próprio dataframe.\n",
        "- `df.dropna(axis=0, how='any', inplace=True)`: remove linhas com qualquer valor faltante.\n",
        "- `pd.to_numeric(..., errors='coerce')`: converte para número, transformando valores inválidos em `NaN`."
      ],
      "id": "-pfjC-MhFDOO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SniOUGkJFDOO"
      },
      "source": [
        "# Substituir valores '?' por NaN\n",
        "df.replace('?', np.nan, inplace=True)\n",
        "\n",
        "# Definir colunas que devem ser numéricas\n",
        "numerical_cols = ['price', 'bore', 'stroke', 'horsepower', 'peak-rpm']\n",
        "for col in numerical_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Remover registros com valores nulos\n",
        "df.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "print(\"\\nTamanho do dataframe após remoção de NaNs:\", df.shape)\n",
        "print(\"\\nDescrição estatística após limpeza:\")\n",
        "display(df.describe())\n",
        "\n",
        "print(\"\\nVerificar tipos de dados pós-limpeza:\")\n",
        "print(df.dtypes)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "SniOUGkJFDOO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnIsjJl7FDOO"
      },
      "source": [
        "## 4. Criação da Variável Alvo (Categorização do Preço)\n",
        "*Definimos duas faixas de corte (percentis de 33% e 66%) para classificar o preço em: 'low', 'medium' ou 'high'.*\n",
        "\n",
        "**Funções e argumentos**:\n",
        "- `np.percentile(df['price'], 33)`: retorna o valor do 33º percentil da coluna `price`.\n",
        "- `df.apply(...)`: aplica uma função em cada valor da coluna `price`."
      ],
      "id": "dnIsjJl7FDOO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGGHrW9ZFDOO"
      },
      "source": [
        "# Calcular os percentis 33 e 66\n",
        "p33 = np.percentile(df['price'], 33)\n",
        "p66 = np.percentile(df['price'], 66)\n",
        "\n",
        "print(f\"\\nValor do 33o percentil: {p33:.2f}\")\n",
        "print(f\"Valor do 66o percentil: {p66:.2f}\")\n",
        "\n",
        "def categorize_price(x):\n",
        "    if x <= p33:\n",
        "        return 'low'\n",
        "    elif x <= p66:\n",
        "        return 'medium'\n",
        "    else:\n",
        "        return 'high'\n",
        "\n",
        "df['price_category'] = df['price'].apply(categorize_price)\n",
        "\n",
        "print(\"\\nDistribuição das categorias de preço:\")\n",
        "display(df['price_category'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "jGGHrW9ZFDOO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dfCGTNzFDOP"
      },
      "source": [
        "## 5. Análise Exploratória Básica\n",
        "*Exibimos um histograma para visualizar a distribuição de preços e um mapa de calor das correlações entre variáveis numéricas.*\n",
        "\n",
        "**Funções e argumentos**:\n",
        "- `sns.histplot(...)`: gera um histograma com opção de curva *kde*.\n",
        "- `sns.heatmap(...)`: mostra uma matriz de correlação com coloração, `annot=True` exibe valores numéricos.\n",
        "- `corr()`: calcula a correlação de Pearson entre as colunas numéricas de um DataFrame."
      ],
      "id": "7dfCGTNzFDOP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ftsaOVdFDOS"
      },
      "source": [
        "# Histograma do preço\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(df['price'], kde=True, color='green')\n",
        "plt.title('Distribuição do Preço (Histograma)')\n",
        "plt.xlabel('Preço')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()\n",
        "\n",
        "# Mapa de calor de correlação\n",
        "numerical_cols_2 = ['wheel-base','length','width','height','curb-weight',\n",
        "                    'engine-size','bore','stroke','compression-ratio','horsepower',\n",
        "                    'peak-rpm','city-mpg','highway-mpg','price']\n",
        "corr_matrix = df[numerical_cols_2].corr()\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='YlGnBu', fmt=\".2f\")\n",
        "plt.title('Mapa de Calor de Correlação')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1ftsaOVdFDOS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JY8SXAOFDOT"
      },
      "source": [
        "## 6. Conversão de Variáveis Categóricas em *Dummies*\n",
        "*Usamos `pd.get_dummies` para criar variáveis binárias (0/1) a partir de colunas categóricas.*\n",
        "\n",
        "**Funções e argumentos**:\n",
        "- `pd.get_dummies(df[cat_cols], drop_first=True)`: cria colunas *dummy* para cada categoria, removendo a primeira (referência) para evitar a *dummy trap*.\n",
        "- `pd.concat([...], axis=1)`: concatena dataframes lado a lado."
      ],
      "id": "-JY8SXAOFDOT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH8BO20QFDOT"
      },
      "source": [
        "# Definir colunas categóricas\n",
        "cat_cols = ['make','fuel-type','aspiration','num-of-doors','body-style',\n",
        "            'drive-wheels','engine-location','engine-type','num-of-cylinders',\n",
        "            'fuel-system']\n",
        "\n",
        "# Criar dummies\n",
        "df_dummies = pd.get_dummies(df[cat_cols], drop_first=True)\n",
        "df = pd.concat([df, df_dummies], axis=1)\n",
        "\n",
        "# Remover as colunas categóricas originais\n",
        "df.drop(columns=cat_cols, inplace=True)\n",
        "\n",
        "print(\"\\nExemplo das colunas dummy:\")\n",
        "display(df_dummies.head(5))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "yH8BO20QFDOT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL0laytgFDOT"
      },
      "source": [
        "## 7. Normalização das Variáveis Numéricas\n",
        "*Aplicamos `MinMaxScaler` para escalar cada coluna numérica para o intervalo [0,1].*\n",
        "\n",
        "**Funções e argumentos**:\n",
        "- `MinMaxScaler(feature_range=(0,1))`: (padrão) transforma cada valor x em (x - min) / (max - min).\n",
        "- `scaler.fit_transform(...)`: primeiro *aprende* (fit) os valores mínimo e máximo das colunas, depois transforma os dados."
      ],
      "id": "PL0laytgFDOT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBkhddjhFDOT"
      },
      "source": [
        "# Selecionar as colunas numéricas para normalizar (exceto a nova coluna price_category)\n",
        "num_cols_for_scaling = ['wheel-base','length','width','height','curb-weight',\n",
        "                        'engine-size','bore','stroke','compression-ratio',\n",
        "                        'horsepower','peak-rpm','city-mpg','highway-mpg','price']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df[num_cols_for_scaling] = scaler.fit_transform(df[num_cols_for_scaling])\n",
        "\n",
        "print(\"\\nExemplo das variáveis normalizadas:\")\n",
        "display(df[num_cols_for_scaling].head())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "SBkhddjhFDOT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZsurHx1FDOT"
      },
      "source": [
        "## 8. Definição de X e y\n",
        "*O vetor de rótulos (`y`) será `price_category`. Em `X`, removemos tanto `price_category` quanto `price` (pois o modelo deve prever a categoria sem ver o preço real).*\n",
        "\n",
        "**Funções e argumentos**:\n",
        "- `df.drop(columns=[...])`: remove colunas específicas do dataframe."
      ],
      "id": "aZsurHx1FDOT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Gr0dMsJFDOT"
      },
      "source": [
        "y = df['price_category'].values\n",
        "X = df.drop(columns=['price_category','price'])\n",
        "\n",
        "print(f\"\\nDimensão de X: {X.shape}\")\n",
        "print(f\"Dimensão de y: {y.shape}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6Gr0dMsJFDOT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkKxeOMdFDOU"
      },
      "source": [
        "## 9. Seleção de Variáveis com RFE\n",
        "*Usamos `LogisticRegression` como estimador para o RFE, a fim de selecionar as 10 melhores variáveis (características) para classificar `price_category`.*\n",
        "\n",
        "**Funções e argumentos**:\n",
        "- `LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500)`: modelo de regressão logística multiclasse, usando o método `lbfgs` e limite de 500 iterações.\n",
        "- `RFE(estimator=..., n_features_to_select=10)`: elimina recursivamente as variáveis menos importantes, até restarem 10 selecionadas.\n",
        "- `rfe.fit(X, y)`: executa o processo de seleção no conjunto de dados.\n",
        "- `rfe.support_`: array booleano indicando quais colunas foram mantidas."
      ],
      "id": "GkKxeOMdFDOU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V7z3WT1FDOU"
      },
      "source": [
        "base_estimator = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500)\n",
        "rfe = RFE(estimator=base_estimator, n_features_to_select=10)\n",
        "\n",
        "rfe.fit(X, y)\n",
        "\n",
        "selected_columns = X.columns[rfe.support_]\n",
        "print(\"\\nColunas selecionadas pelo RFE:\")\n",
        "print(selected_columns)\n",
        "\n",
        "# Criar novo X somente com as colunas selecionadas\n",
        "X_selected = X[selected_columns]"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1V7z3WT1FDOU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0QQfhbDFDOU"
      },
      "source": [
        "## 10. Divisão em Treino e Teste\n",
        "*Dividimos o dataset em 70% treino e 30% teste, estratificando pela variável alvo (`price_category`).*\n",
        "\n",
        "**Funções e argumentos**:\n",
        "- `train_test_split(X_selected, y, test_size=0.30, stratify=y, random_state=42)`: separa dados em treino e teste; 30% vai para teste. `stratify=y` mantém a proporção de classes, `random_state=42` define uma semente para reprodutibilidade."
      ],
      "id": "B0QQfhbDFDOU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95sV9wpKFDOU"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_selected, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\nDimensões do conjunto de treino:\", X_train.shape, y_train.shape)\n",
        "print(\"Dimensões do conjunto de teste:\", X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "95sV9wpKFDOU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cThaL7mCFDOU"
      },
      "source": [
        "## 11. Treinamento da Rede Neural (MLPClassifier)\n",
        "*Definimos um `MLPClassifier` com duas camadas ocultas (16 e 8 neurônios), função de ativação `'relu'` e solver `'adam'`. Podemos ajustar o número de iterações (`max_iter`) caso seja necessário mais tempo para convergir.*\n",
        "\n",
        "**Funções e argumentos**:\n",
        "- `MLPClassifier(hidden_layer_sizes=(16,8), activation='relu', solver='adam', max_iter=2000, random_state=42)`: cria uma rede neural com 2 camadas ocultas, cada qual contendo 16 e 8 neurônios, respectivamente. A função de ativação `'relu'` (Rectified Linear Unit) é uma das mais usadas em redes neurais. O `'adam'` é um método de otimização. `max_iter=2000` define o número máximo de épocas (iterações) de treinamento. `random_state=42` garante reprodutibilidade dos resultados.\n",
        "- `mlp.fit(X_train, y_train)`: treina (ajusta) os pesos da rede neural nos dados de treino."
      ],
      "id": "cThaL7mCFDOU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uBTx0QUFDOU"
      },
      "source": [
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(16, 8),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=2000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "mlp.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7uBTx0QUFDOU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoyfKfWlFDOU"
      },
      "source": [
        "## 12. Avaliação do Modelo\n",
        "*Utilizamos a rede treinada para prever no conjunto de teste. Avaliamos por meio da matriz de confusão, relatório de classificação (contendo precisão, recall e F1), e acurácia geral.*\n",
        "\n",
        "**Funções e argumentos**:\n",
        "- `mlp.predict(X_test)`: gera as previsões de classe para cada amostra do conjunto de teste.\n",
        "- `confusion_matrix(y_test, y_pred, labels=[...])`: produz uma matriz de confusão, onde `labels` define a ordem das classes.\n",
        "- `sns.heatmap(...)`: usada para plotar a matriz de confusão com anotações.\n",
        "- `classification_report(...)`: exibe precisão, recall, F1-score e suporte para cada classe.\n",
        "- `accuracy_score(...)`: exibe a fração de acertos (quantos % das amostras foram classificadas corretamente)."
      ],
      "id": "zoyfKfWlFDOU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT7YcJX6FDOV"
      },
      "source": [
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "print(\"\\nMatriz de Confusão:\")\n",
        "cm = confusion_matrix(y_test, y_pred, labels=['low','medium','high'])\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['low','medium','high'],\n",
        "            yticklabels=['low','medium','high'])\n",
        "plt.title(\"Matriz de Confusão (Teste) - MLP\")\n",
        "plt.xlabel(\"Previsto\")\n",
        "plt.ylabel(\"Verdadeiro\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nRelatório de Classificação:\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Acurácia no teste: {acc*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "KT7YcJX6FDOV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72aySOuHFDOV"
      },
      "source": [
        "## 13. Conclusões\n",
        "*Resumo dos resultados e possíveis pontos de melhoria.*\n",
        "\n",
        "- **Rede Neural MLP**: demonstrou capacidade de separar bem as faixas de preço dos automóveis (baixo, médio, alto) após o pré-processamento e seleção de variáveis.\n",
        "- **RFE**: mesmo utilizando uma regressão logística como estimador, ajudou a reduzir a dimensionalidade, selecionando os atributos mais relevantes.\n",
        "- **Métricas**: acurácia, precisão, recall e F1-score podem indicar se o modelo está equilibrado entre as classes. Uma boa prática é analisar se há confusão excessiva entre *low* e *medium*, ou entre *medium* e *high*.\n",
        "- Poderíamos **ajustar hiperparâmetros** (como número de neurônios, camadas, função de ativação, taxa de aprendizado) para melhorar ainda mais o desempenho.\n",
        "- **Interpretação**: apesar de redes neurais serem modelos mais \"caixa-preta\", a abordagem utilizada (RFE + MLP) ainda pode fornecer insights sobre quais *features* são mais importantes (via RFE) e como afetam a classificação de preço."
      ],
      "id": "72aySOuHFDOV"
    }
  ]
}